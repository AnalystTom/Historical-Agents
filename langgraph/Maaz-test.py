# -*- coding: utf-8 -*-
"""Copy of debate_agent - amended.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kf_MassRUgjCW-sQXmVysuCJoyAsVkQO
"""


from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from typing import Any, List, TypedDict
import os
import time

# Your other imports
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_groq import ChatGroq
from langgraph.graph import MessagesState
from langgraph.graph.state import StateGraph
from langgraph.checkpoint.memory import MemorySaver

# [Add other necessary imports from your original code]


from IPython.display import Image, display, Markdown
import textwrap
import getpass

from typing import Any, Annotated, List, TypedDict
from pydantic import BaseModel, Field

from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.document_loaders import WikipediaLoader
from langchain_community.retrievers import WikipediaRetriever
from langchain_groq import ChatGroq

from langgraph.graph import MessagesState
from langgraph.graph.state import StateGraph, END, START
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph.message import add_messages

# google_api_key = userdata.get('GOOGLE_API_KEY')

# model = ChatGoogleGenerativeAI(model="gemini-1.5-flash",
#                               api_key=google_api_key
#                               )

# # model.invoke("Test")


app = FastAPI()

# Handle CORS (Cross-Origin Resource Sharing)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, specify allowed origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Set your API keys (ensure they are securely stored)
os.environ["TAVILY_API_KEY"] = "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"
os.environ["GROQ_API_KEY"] = "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"

# Initialize the model
model = ChatGroq(
    model="llama-3.2-1b-preview",
    verbose=True,
    temperature=0.5,
    api_key=os.environ["GROQ_API_KEY"]
)

model.invoke('TEST').content

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")


_set_env("TAVILY_API_KEY")

memory = MemorySaver()

class State(TypedDict):
  topic: str
  pro_debator: str
  anti_debator: str
  greetings: str
  analysis: str
  pro_debator_response: str
  anti_debator_response: str
  context: Annotated[list, add_messages]
  debate: Annotated[list, add_messages]
  debate_history: List[str]
  iteration: int
  max_iteration: int

class SearchQuery(BaseModel):
  search_query: str = Field(description="The search query for retrieval")

structure_llm = model.with_structured_output(SearchQuery)
structure_llm

def measure_time(node_function):
    """Decorator to measure and log the execution time of a node function."""
    def wrapper(state, *args, **kwargs):
        start_time = time.time()
        print(f"Starting node: {node_function.__name__}")
        result = node_function(state, *args, **kwargs)
        end_time = time.time()
        elapsed_time = end_time - start_time
        print(f"Node {node_function.__name__} completed in {elapsed_time:.2f} seconds.\n")

        # Optionally store in state for later analysis
        if "node_times" not in state:
            state["node_times"] = {}
        state["node_times"][node_function.__name__] = elapsed_time

        return result
    return wrapper

@measure_time
def greeting_node(state: State):
  """LangGraph node that greets the debators and introduces them"""
  print("Greeting Node")
  topic = state['topic']
  pro_debator = state['pro_debator']
  anti_debator = state['anti_debator']

  prompt = f"""You are hosting a debate between {pro_debator} and {anti_debator}
            on the topic {topic}. {pro_debator} is pro while {anti_debator} is
            against. You have to introduce the topic and debators to the audience.
            Your response should be short and conversational
            """

  greetings = model.invoke(prompt).content
  return {"greetings": greetings}


@measure_time
def analyzer_node(state: State):
    """LangGraph node that analyzes the latest argument for web search"""
    print("Analyzer Node")
    topic = state['topic']
    debate = state['debate']
    pro_debator = state['pro_debator']
    anti_debator = state['anti_debator']
    last_message = debate[-1]
    analysis_prompt = None
    if isinstance(last_message, HumanMessage):
        # Generate a prompt for a HumanMessage (pro-debator)
        print("Analyzing for Anti Debator")
        analysis_prompt = f"""
        Analyze the latest argument made by the pro-debator {pro_debator}  on the topic "{topic}".
        Focus on its strengths, weaknesses, and logical coherence. Write a short and concise
        analytical guidance that can be used for web search to help {anti_debator} better answer the argument
        and more completely support their stance on the topic {topic}. Keep the analysis as short as possible
        without losing quality.
        **Pro-Debator's Argument:**
        {last_message.content}
        """

    elif isinstance(last_message, AIMessage):
        # Generate a prompt for an AIMessage (anti-debator)
        print("Analyzing for Pro Debator")
        analysis_prompt = f"""
        Analyze the latest counterargument made by the anti-debator {anti_debator} on the topic "{topic}".
        Identify key points of contention and evaluate their validity.  Write a short and concise
        analytical guidance that can be used for web search to help {anti_debator} to effectively refute these arguments
        and more completely support their stance on the topic {topic}. Keep the analysis as short as possible
        without losing quality.
        **Anti-Debator's Counterargument:**
        {last_message.content}
        """

    analysis = model.invoke(analysis_prompt).content
    return {"analysis": analysis}


@measure_time
def search_web(state: State):
    """LangGraph node to search the web using Tavily Search API and append the results to context."""
    analysis = state['analysis']

    context = state['context']

    # Generate Search Query
    search_query = model.invoke(
        f"Generate a web search query using analysis {analysis} and debate history {state['debate_history']}. the search query should be no longer than 3 sentences"
    ).content
    print("Tavily Search Query:", search_query)
    tavily_search = TavilySearchResults(
                      max_results=2,
                      include_answer=True,
                      include_raw_content=True,
                      # search_depth="advanced",
                      # include_domains = []
                      # exclude_domains = []
                  )
    search_docs = tavily_search.invoke(search_query)
    print("search_docs:", search_docs)

    # Check if `search_docs` contains valid dictionaries
    if isinstance(search_docs, list) and all(isinstance(doc, dict) for doc in search_docs):
        formatted_search_docs = "\n\n---\n\n".join(
            [
                f"**URL:** {doc.get('url', 'No URL')}\n**Content:** {doc.get('content', 'No Content')}"
                for doc in search_docs
            ]
        )
    elif isinstance(search_docs, list) and all(isinstance(doc, str) for doc in search_docs):
        formatted_search_docs = "\n\n---\n\n".join(search_docs)
    else:
        formatted_search_docs = "Search results are in an unexpected format."

    # Append to context
    context.append(formatted_search_docs)
    return {"context": context}



@measure_time
def search_wikipedia(state: State):
    """Retrieve docs from Wikipedia using WikipediaRetriever"""
    print("Searching Wikipedia")

    # Analysis and debate context
    analysis = state['analysis']
    debate_history = state['debate_history']
    search_query = model.invoke(
        f"Generate a wikipedia search query using analysis {analysis} and debate history {state['debate_history']}. the search query should be no longer than 3 sentences"
    ).content
    print("Wikipedia Search Query:", search_query)

    # WikipediaRetriever setup
    retriever = WikipediaRetriever()
    search_docs = retriever.get_relevant_documents(search_query)

    # Format the results
    formatted_search_docs = "\n\n---\n\n".join(
        [
            f"<Document title='{doc.metadata.get('title', 'Unknown Title')}'/>\n{doc.page_content}\n</Document>"
            for doc in search_docs
        ]
    )

    print(f"Wikipedia DOcs: {formatted_search_docs}")
    return {"context": [formatted_search_docs]}


@measure_time
def router(state: State):
    """LangGraph node that routes to the appropriate search function"""
    debate_history = state["debate_history"]
    if debate_history == []:
        return "Pro Debator"
    else:
      return "Analyzer"

def iteration_router(state: State):
    """Routes the flow based on the current iteration and max_iteration"""

    if state['iteration'] <= state['max_iteration']:
        print(f"Iteration Round: {state['iteration']}")
        state['iteration'] = state['iteration'] + 1
        return "Analyzer"
    else:
        # End the debate
        return END

@measure_time
def analyzer_router(state: State):
    """Function that routes to the appropriate next node"""
    debate = state['debate']
    last_message = debate[-1]
    if isinstance(last_message, AIMessage):
        return "Pro Debator"  # Pro Debator responds to the anti-debator's argument
    else:
        return "Anti Debator"  # Anti Debator responds to the pro-debator's argument

@measure_time
def pro_debator_node(state: State):
    """LangGraph node that represents the pro debator"""

    print("Pro Debator Node")

    # Extract state variables
    topic = state['topic']
    anti_debator_response = state['anti_debator_response']
    pro_debator = state['pro_debator']
    anti_debator = state['anti_debator']
    debate_history = state['debate_history']
    debate = state['debate']

    # Initial debate opening
    if anti_debator_response is None and debate == []:
        prompt_template = """Role: You are {pro_debator} in a formal debate setting.

        Topic: "{topic}"

        Instructions:
        - Present a strong opening argument supporting your historical position on {topic}
        - Use your characteristic speaking style, mannerisms, and common phrases
        - Draw from your known policy positions and public statements
        - Keep the response focused on policy and facts
        - Speak in first person as {pro_debator}

        Format your response as a natural speaking debate opening, addressing the moderator and audience appropriately.
        """

        system_message = SystemMessage(content=prompt_template.format(
            topic=topic,
            pro_debator=pro_debator,
            anti_debator=anti_debator
        ))

        messages = [system_message]

    # Responding to opponent
    else:
        context = state['context']
        prompt_template = """Role: You are {pro_debator} in an ongoing debate.

        Topic: "{topic}"

        Recent opponent ({anti_debator}) statement:
        {anti_debator_response}

        Debate context:
        {debate_history}

        Additional context:
        {context}

        Instructions:
        - Directly address and counter the points made by {anti_debator}
        - Maintain your known position and policy stance on {topic}
        - Use your characteristic speaking style and mannerisms
        - Support arguments with specific examples and facts
        - Keep response concise (2-4 sentences)
        - Speak in first person as {pro_debator}

        Format your response as a natural rebuttal in a debate setting.
        """

        system_message = SystemMessage(content=prompt_template.format(
            topic=topic,
            pro_debator=pro_debator,
            anti_debator=anti_debator,
            debate_history=debate_history,
            anti_debator_response=anti_debator_response,
            context=context
        ))

        messages = [system_message]

    # Generate response using the model
    response = model.invoke(messages)
    pro_debator_response_content = response.content

    # Create a HumanMessage with the response content
    pro_debator_response = HumanMessage(
        content=f"{pro_debator}: {pro_debator_response_content}",
        name="pro_response"
    )

    debate.append(pro_debator_response)
    return {"pro_debator_response": pro_debator_response, "debate": debate}

@measure_time
def anti_debator_node(state: State):
    """LangGraph node that represents the anti debator"""
    print("Anti Debator Node")

    # Extract state variables
    topic = state['topic']
    pro_debator_response = state['pro_debator_response']
    pro_debator = state['pro_debator']
    anti_debator = state['anti_debator']
    debate_history = state['debate_history']
    debate = state['debate']
    context = state['context']

    # Get the latest pro debator response
    latest_pro_response = pro_debator_response.content if pro_debator_response else ""

    prompt_template = """Role: You are {anti_debator} participating in a formal debate.

    Topic: "{topic}"

    Latest statement by {pro_debator}:
    {latest_pro_response}

    Debate history:
    {debate_history}

    Available context:
    {context}

    Instructions:
    1. Speaking Style:
       - Use your characteristic speaking patterns, catchphrases, and mannerisms
       - Maintain your well-known personality traits and debate style
       - Address both the moderator and your opponent as appropriate

    2. Content Guidelines:
       - Directly counter the points made by {pro_debator}
       - Present your known stance and policy positions on {topic}
       - Use specific examples and facts to support your arguments
       - Draw from your public statements and previous positions on this issue
       - Keep response focused and concise (2-4 sentences)

    3. Debate Strategy:
       - Challenge the assumptions in your opponent's argument
       - Highlight any inconsistencies or weaknesses
       - Present alternative solutions or perspectives
       - Emphasize the practical implications of your position

    Remember: Stay in character as {anti_debator} throughout your response. Format your response as a natural rebuttal in a debate setting, speaking in first person.
    """

    # Create system message
    system_message = SystemMessage(content=prompt_template.format(
        topic=topic,
        pro_debator=pro_debator,
        latest_pro_response=latest_pro_response,
        anti_debator=anti_debator,
        debate_history=debate_history,
        context=context
    ))

    # Generate response using the model
    anti_debator_response_content = model.invoke([system_message]).content

    # Create an AIMessage with the response content
    anti_debator_response = AIMessage(
        content=f"{anti_debator}: {anti_debator_response_content}",
        name="anti_response"  # Fixed the name to be "anti_response" instead of "pro_response"
    )

    # Update debate history
    debate.append(anti_debator_response)

    return {
        "anti_debator_response": anti_debator_response,
        "debate": debate
    }

@measure_time
def debate_summarizer_node(state: State):
  """LangGraph node that summarizes the exchange of arguments between debator
  and append to history for future consideration
  """
  pro_debator = state['pro_debator']
  anti_debator = state['anti_debator']
  debate_history = state['debate_history']
  anti_debator_response = state['anti_debator_response']
  pro_debator_response = state['pro_debator_response']
  prompt = """
            Summarize the conversation between the pro {pro_debator} and anti debator {anti_debator},
            highlighting the key points of their arguments and discarding unnecessary points. The
            summary should be concise and brief, with high quality.
            **Instructions:**
            * Focus on the core arguments presented by both sides.
            * Identify the main points of agreement and disagreement.
            * Provide a clear and objective overview of the debate.
            * Avoid including irrelevant details or repetitive information.
            * Ensure that the summary is easy to understand and informative.
            * The summary should be approximately 1.
            **Pro Debator:**
            {pro_debator_response}
            **Anti Debator:**
            {anti_debator_response}
          """
  system_message = prompt.format(
                      pro_debator=pro_debator,
                      pro_debator_response=anti_debator_response,
                      anti_debator=anti_debator,
                      anti_debator_response=anti_debator_response,
                    )
  summary = model.invoke(system_message).content
  debate_history.append(summary)
  return {"debate_history": debate_history}



builder = StateGraph(State)

# Add nodes
builder.add_node("Greetings", greeting_node)
builder.add_node("Pro Debator", pro_debator_node)
builder.add_node("Analyzer", analyzer_node)
builder.add_node("Anti Debator", anti_debator_node)
builder.add_node("Debate Summarizer", debate_summarizer_node)

# Add edges
builder.add_edge(START, "Greetings")
builder.add_conditional_edges("Greetings", router, ['Analyzer', 'Pro Debator'])
builder.add_conditional_edges("Analyzer", analyzer_router, ["Pro Debator", "Anti Debator"])
builder.add_edge("Pro Debator", "Analyzer")
builder.add_edge("Anti Debator", "Debate Summarizer")
builder.add_edge("Debate Summarizer", END)

# Compile the graph
debator = builder.compile(checkpointer=memory).with_config(run_name="Create podcast")

# Display the graph
display(Image(debator.get_graph().draw_mermaid_png()))

# state = {
#     "topic": "Ukraine War",
#     "pro_debator": "Joe Biden",
#     "anti_debator": "Donald Trump",
#     "greetings": "",
#     "analysis": "",
#     "pro_debator_response": "",
#     "anti_debator_response": "",
#     "context": [],
#     "debate": [],
#     "debate_history": [],
#     "iteration": 0,
#     "max_iteration": 1
# }

# thread = {"configurable": {"thread_id": "32"}}
# result = debator.invoke(state, thread)
# result

@app.post("/trigger_workflow")
async def trigger_workflow(request: Request):
    data = await request.json()
    debate_topic = data.get('debate_topic')
    debater1 = data.get('debater1')
    debater2 = data.get('debater2')
    max_iterations = data.get('max_iterations', 3)

    # Initialize state
    state = {
        "topic": debate_topic,
        "pro_debator": debater1,
        "anti_debator": debater2,
        "greetings": "",
        "analysis": "",
        "pro_debator_response": None,
        "anti_debator_response": None,
        "context": [],
        "debate": [],
        "debate_history": [],
        "iteration": 0,
        "max_iteration": max_iterations
    }

    # Add the required 'configurable' keys
    thread = {"configurable": {"thread_id": "unique_thread_id"}}

    # Run the debate agent
    result = debator.invoke(state, thread)

    # Prepare the conversation history in JSON format
    conversation = []
    for message in result['debate']:
        # Determine the speaker based on the message type
        if isinstance(message, HumanMessage):
            speaker = debater1
        elif isinstance(message, AIMessage):
            speaker = debater2
        else:
            speaker = "System"

        conversation.append({
            'speaker': speaker,
            'content': message.content
        })

    response = {
        'greetings': result['greetings'],
        'conversation': conversation,
        'debate_history': result['debate_history']
    }

    return response



if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)