# -*- coding: utf-8 -*-
"""Copy of debate_agent - amended.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kf_MassRUgjCW-sQXmVysuCJoyAsVkQO
"""

# LangChain + Monto Carlo Search Tree( )
# Tracing  LangSmith,
# Debate search
# Tooling
# Output presentation
# Reduce hallunincatation

from textwrap3 import indent, dedent

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from typing import Any, List, TypedDict
import os
import time
from pymongo import MongoClient
from langchain.text_splitter import CharacterTextSplitter
from sentence_transformers import SentenceTransformer
from datetime import datetime
# Your other imports
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_groq import ChatGroq
from langgraph.graph import MessagesState
from langgraph.graph.state import StateGraph
from langgraph.checkpoint.memory import MemorySaver

# [Add other necessary imports from your original code]


from IPython.display import Image, display, Markdown
import textwrap
import getpass

from typing import Any, Annotated, List, TypedDict
from pydantic import BaseModel, Field

from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.document_loaders import WikipediaLoader
from langchain_community.retrievers import WikipediaRetriever
from langchain_groq import ChatGroq

from langgraph.graph import MessagesState
from langgraph.graph.state import StateGraph, END, START
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph.message import add_messages

# google_api_key = userdata.get('GOOGLE_API_KEY')

# model = ChatGoogleGenerativeAI(model="gemini-1.5-flash",
#                               api_key=google_api_key
#                               )

# # model.invoke("Test")


app = FastAPI()

# Handle CORS (Cross-Origin Resource Sharing)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, specify allowed origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Set your API keys (ensure they are securely stored)
os.environ["TAVILY_API_KEY"] = "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"
os.environ["GROQ_API_KEY"] = "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"


# Initialize the model
model = ChatGroq(
    model="llama-3.2-1b-preview",
    verbose=True,
    temperature=0.5,
    api_key=os.environ["GROQ_API_KEY"]
)

model.invoke('TEST').content

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")


_set_env("TAVILY_API_KEY")

memory = MemorySaver()

# class State(TypedDict):
#   topic: str
#   pro_debator: str
#   anti_debator: str
#   greetings: str
#   planning: str
#   pro_debator_response: str
#   anti_debator_response: str
#   context: Annotated[list, add_messages]
#   debate: Annotated[list, add_messages]
#   debate_history: List[str]
#   iteration: int
#   max_iteration: int

class State(TypedDict):
  topic: str
  pro_debator: str
  anti_debator: str
  greetings: str
  pro_debator_response: str
  anti_debator_response: str
  context: Annotated[list, add_messages]
  debate: Annotated[list, add_messages]
  debate_history: List[str]
  planner: str
  winner: str
  iteration: int
  max_iteration: int

class SearchQuery(BaseModel):
  search_query: str = Field(description="The search query for retrieval")

structure_llm = model.with_structured_output(SearchQuery)
structure_llm

def measure_time(node_function):
    """Decorator to measure and log the execution time of a node function."""
    def wrapper(state, *args, **kwargs):
        start_time = time.time()
        print(f"Starting node: {node_function.__name__}")
        result = node_function(state, *args, **kwargs)
        end_time = time.time()
        elapsed_time = end_time - start_time
        print(f"Node {node_function.__name__} completed in {elapsed_time:.2f} seconds.\n")

        # Optionally store in state for later analysis
        if "node_times" not in state:
            state["node_times"] = {}
        state["node_times"][node_function.__name__] = elapsed_time

        return result
    return wrapper

@measure_time
def greeting_node(state: State):
  """LangGraph node that greets the debators and introduces them"""
  print("Greeting Node")
  topic = state['topic']
  pro_debator = state['pro_debator']
  anti_debator = state['anti_debator']

  prompt = f"""
        Welcome the audience to a lively debate between {pro_debator} and {anti_debator} on {topic}.

        {pro_debator} champions the "Pro" side, and {anti_debator} defends the "Against" side.
        Introduce them warmly in under 40 words, keeping it fun and engaging!
            """

  greetings = model.invoke(prompt).content
  return {"greetings": greetings}

from pymongo import MongoClient
from langchain.text_splitter import CharacterTextSplitter
from sentence_transformers import SentenceTransformer
from datetime import datetime
from transformers import pipeline
from datetime import datetime

# Connect to MongoDB
client = MongoClient("mongodb+srv://user:BUiIZW9wSnqgPbhN@histcluster.lijlj.mongodb.net/personaDB?retryWrites=true&w=majority")
db = client["personaDB"]
collection = db["conversionChunkLog"]

# Load embedding model (offline)
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")  # Replace with your chosen offline model

def store_debate_summary(turn, role, position, text):
        # Initialize the summarization pipeline with a smaller model
    summarizer = pipeline("summarization", model="t5-small", tokenizer="t5-small")

    # Summarize the text into a maximum of 10 sentences
    summary = summarizer(text, max_length=130, min_length=50, do_sample=False)[0]['summary_text']

    # Create the document
    document = {
        "turn": turn,
        "role": role,
        "summarized_text": summary,
        "position": position,
        "timestamp": datetime.utcnow()
    }

    # Insert the document into MongoDB
    collection.insert_one(document)




@measure_time
def planning_node(state: State):
    """LangGraph node that analyzes the latest argument for web search"""
    topic = state['topic']
    pro_debator = state['pro_debator']
    anti_debator = state['anti_debator']
    last_message = state["debate"][-1]
    planning_prompt = None

    system_message = ""

    if isinstance(last_message, HumanMessage):
      print("Planning for Anti Debator")
      planning_prompt = """
        You are an expert in debate strategy. Your task is to help the anti-debator
        {anti_debator} craft
        a compelling counter-argument to the pro-debator's, {pro_debator}, arguments on the debate topic:
        {topic}.
        Here's the information you have:
        * **Pro-Debator's Argument:** {last_message}
        Generate an actionable plan with the following structure:
        **1. Identify Weaknesses:** Analyze the pro-debator's argument. Pinpoint logical
        fallacies, weak points, unsupported claims, or areas where more evidence is needed.
        **2. Research and Evidence Gathering:** Suggest specific research avenues to find
        evidence that refutes the pro-debator's argument.  Provide concrete examples of
        sources and keywords.
        **3. Counter-Argument Formulation:** Outline the main points of a counter-argument.
        Each point should directly address a weakness in the pro-debator's argument and be
        supported by the suggested research.
        **4. Rebuttals:** Anticipate the pro-debator's possible rebuttals and suggest
        preemptive counter-rebuttals.
        **5. Presentation Strategy:** Outline how to present the counter-argument
        effectively:
            * Should the anti-debator focus on emotion or logic?
            * What rhetorical devices would be effective?
            * How to present the evidence concisely and persuasively?
        Example Output:
        **1. Identify Weaknesses:** The pro-debator's argument relies on a study from
        2010, which may be outdated.  They also don't address the economic impact of
        their proposal.

        **2. Research and Evidence Gathering:** Search for more recent studies on the
        topic. Look for economic analyses of similar proposals. Search terms: "[topic]
        economic impact," "[topic] recent studies," etc.  Look for credible sources
        such as peer-reviewed journals.
        **3. Counter-Argument Formulation:**
            * Point 1: The 2010 study is outdated and newer research contradicts its findings.
            * Point 2: The proposal has significant negative economic consequences.
        **4. Rebuttals:** The pro-debator might argue that the newer studies are biased.
        Prepare to address this by presenting evidence of the studies' methodology and
        peer review.
        **5. Presentation Strategy:** Emphasize the economic impact and present the data
        visually. Maintain a logical, calm demeanor. Use statistics and specific examples
        instead of generalizations.

        Ensure the plan is specific to the given information.
      """

      system_message = planning_prompt.format(
          topic=topic,
          anti_debator=anti_debator,
          pro_debator = pro_debator,
          last_message=last_message,
      )

    elif isinstance(last_message, AIMessage):
      # Analysis for an AIMessage (anti-debator's counterargument)
      print("Analyzing for Pro Debator")
      planning_prompt = """
        You are an expert debate strategist tasked with formulating a
        counter-argument
        against an opponent's position on a given topic.  Your goal is to
        create an actionable plan to devise a compelling and effective
        counter-argument for {anti_debator} against {pro_debator}.
        Given the following information:
        1. **Topic:** {topic}
        2. **Anti-Debator's Argument:** {last_message}
        4. **Desired Outcome:** Develop a counter-argument that effectively
        refutes the opponent's claims, strengthens your own position, and
        persuades the audience.


        **Develop an actionable plan that includes the following:**

        * **Identify Key Weaknesses:** Analyze the opponent's argument for
        logical fallacies, weak points, unsupported claims, or inconsistencies.
        List at least 3 key weaknesses.
        * **Research & Evidence Gathering:** Specify relevant areas of research,
        data sources, or examples that can be used to support your counter-argument.
        * **Counter-Argument Formulation:**  Outline the structure of your
        counter-argument.  Include the key points you will make and how they
        directly address the weaknesses identified.
        * **Rebuttals:** Anticipate potential rebuttals from the opponent and
        formulate concise responses.
        * **Presentation Strategy:**  Suggest how to effectively present your
        counter-argument, considering factors such as tone, clarity, and
        persuasive language.
        **Deliverable:** A detailed, step-by-step plan that can be used to
        create a powerful and persuasive counter-argument.
      """
      system_message = planning_prompt.format(
          topic=topic,
          anti_debator=anti_debator,
          pro_debator = pro_debator,
          last_message=last_message
      )
    state['planner'] = model.invoke(system_message).content
    return state


@measure_time
def search_web(state: State):
    """LangGraph node that do a DuckDuckGo search and append the results to context."""
    planner = state['planner']
    last_message = state['debate'][-1]

    prompt = f"""
        You are a search query generator for debate.
        Instructions:
        Based on the provided planning of the latest argument and the
        last message in a debate, generate a concise search query (maximum 8 words)
        focused on retrieving statistical and numerical data relevant to the latest prompt.
        Prioritize queries that are likely to yield objective data.
        Planning:
        {planner}

        Last Message:
        {last_message}
      """
    search_query = model.invoke(prompt).content.strip()

    print("DuckDuckGo Search Query:", search_query)

    search = DuckDuckGoSearchResults(backend="news", output_format='list')
    search_result = search.invoke(search_query)
    result = ""
    for entry in search_result:
        print(entry['snippet'])
        result += entry['snippet'] + "\n"

    state['context'].append(result)
    return {"context": state['context']}



@measure_time
def search_wikipedia(state: State):
    """Retrieve docs from Wikipedia using WikipediaRetriever"""

    planner = state['planner']
    last_message = state["debate"][-1]
    pro_debator = state['pro_debator']
    anti_debator = state['anti_debator']
    topic = state['topic']


    search_query_prompt = ""
    if isinstance(last_message, HumanMessage):
      search_query_prompt = f"""
        You are a search assistant generating a concise search query for Wikipedia.
        Task:
        Find the most relevant wikipedia articles for {pro_debator} related to
        the topic {topic} taking into account the following planning:
        {planner}
        Output:
        A single concise search query relevant to the topic.

        Given debater Trump and topic illegal immigration provide Immigration_policy_of_Donald_Trump
        as search query
      """
    elif isinstance(last_message, AIMessage):
      search_query_prompt = f"""
            You are a search assistant generating a concise search query for Wikipedia.
            Task:
            Find the most relevant wikipedia articles for {anti_debator} related to
            the topic {topic}
            Output:
            A single concise search query relevant to the topic.

           Given debater Trump and topic illegal immigration provide Immigration_policy_of_Donald_Trump
          as search query
          """

    search_query = model.invoke(search_query_prompt).content.strip()

    print(f'Search Query: {search_query}\n')

    retriever = WikipediaRetriever()

    search_docs = retriever.invoke(search_query)
    print(f'Search Docs: {search_docs}')

    all_summaries = ""
    for doc in search_docs:
        if 'summary' in doc.metadata:
            all_summaries += doc.metadata['summary'] + "\n\n"

    state['context'].append(all_summaries)
    print(f"Updated Context: {state['context']}")
    return state


def router(state: State):
    """LangGraph node that routes to the appropriate search function"""
    debate_history = state["debate_history"]
    if debate_history == []:
        return "Pro Debator"
    else:
      return "Planner"


def iteration_router(state: State):
    """Routes the flow based on the current iteration and max_iteration"""
    if state['iteration'] >= state['max_iteration']:
        print("Ending the debate as max iteration is reached.")
        return "Winner Decider"
    print(f"Iteration Round: {state['iteration']}")
    state['iteration'] += 1
    return "Planner"


def analyzer_router(state: State):
    """Function that routes to the appropriate next node"""
    debate = state['debate']
    last_message = debate[-1]
    if isinstance(last_message, AIMessage):
        return "Pro Debator"
    else:
        return "Anti Debator"


from pymongo import MongoClient
from langchain.text_splitter import CharacterTextSplitter
from sentence_transformers import SentenceTransformer
from datetime import datetime

# Connect to MongoDB
client = MongoClient("mongodb+srv://user:BUiIZW9wSnqgPbhN@histcluster.lijlj.mongodb.net/personaDB?retryWrites=true&w=majority")
db = client["personaDB"]
collection = db["conversionChunkLog"]

# Load embedding model (offline)
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")  # Replace with your chosen offline model

def store_debate_chunk(turn, role, position, text):
    # Split text into chunks
    text_splitter = CharacterTextSplitter(separator=". ", chunk_size=200, chunk_overlap=50)
    chunks = text_splitter.split_text(text)
    
    # Process each chunk
    for chunk_number, chunk_text in enumerate(chunks, start=1):
        embedding = embedding_model.encode(chunk_text).tolist()
        document = {
            "turn": turn,
            "role": role,
            "chunk_number": chunk_number,
            "chunk_text": chunk_text,
            "position": position,
            "embedding": embedding,
            "timestamp": datetime.utcnow()
        }
        # Insert document into MongoDB
        collection.insert_one(document)

@measure_time
def pro_debator_node(state: State):
    """LangGraph node that represents the pro debator"""
    print("Pro Debator Node")

    # Extract state variables
    topic = state['topic']
    anti_debator_response = state['anti_debator_response']
    pro_debator = state['pro_debator']
    anti_debator = state['anti_debator']
    debate_history = state['debate_history']
    debate = state['debate']
    curr_iter = state['iteration']

    # Initial debate opening
    if anti_debator_response is None and not debate:
        prompt_template = """Role: You are {pro_debator} in a formal debate setting.

        Topic: "{topic}"

        Instructions:
        - Present a strong opening argument supporting your historical position on {topic}
        - Use your characteristic speaking style, mannerisms, and common phrases
        - Draw from your known policy positions and public statements
        - Keep the response focused on policy and facts
        - Speak in first person as {pro_debator}

        Format your response as a natural speaking debate opening, addressing the moderator and audience appropriately.
        """

        system_message = SystemMessage(content=prompt_template.format(
            topic=topic,
            pro_debator=pro_debator,
            anti_debator=anti_debator
        ))
        messages = [system_message]
    else:
        # Responding to opponent
        context = state['context']
        prompt_template = """Role: You are {pro_debator} in an ongoing debate.

        Topic: "{topic}"

        Recent opponent ({anti_debator}) statement:
        {anti_debator_response}

        Debate context:
        {debate_history}

        Additional context:
        {context}

        Instructions:
        - Directly address and counter the points made by {anti_debator}
        - Maintain your known position and policy stance on {topic}
        - Use your characteristic speaking style and mannerisms
        - Support arguments with specific examples and facts
        - Keep response concise (2-4 sentences)
        - Speak in first person as {pro_debator}

        Format your response as a natural rebuttal in a debate setting.
        """

        system_message = SystemMessage(content=prompt_template.format(
            topic=topic,
            pro_debator=pro_debator,
            anti_debator=anti_debator,
            debate_history=debate_history,
            anti_debator_response=anti_debator_response,
            context=context
        ))

        messages = [system_message]

    # Generate response using the model
    response = model.invoke(messages)
    pro_debator_response_content = response.content

    # Store chunks in MongoDB with embeddings
    store_debate_summary(
        turn=curr_iter,
        role="pro_debator",
        position="pro",
        text=pro_debator_response_content
    )

    # Create a HumanMessage with the response content
    pro_debator_response = HumanMessage(
        content=f"{pro_debator}: {pro_debator_response_content}",
        name="pro_response"
    )

    # Update debate history
    debate.append(pro_debator_response)

    return {"pro_debator_response": pro_debator_response, "debate": debate}



@measure_time
def anti_debator_node(state: State):
    """LangGraph node that represents the anti debator"""
    print("Anti Debator Node")

    # Extract state variables
    topic = state['topic']
    pro_debator_response = state['pro_debator_response']
    pro_debator = state['pro_debator']
    anti_debator = state['anti_debator']
    debate_history = state['debate_history']
    debate = state['debate']
    context = state['context']
    curr_iter = state['iteration']

    # Get the latest pro debator response
    latest_pro_response = pro_debator_response.content if pro_debator_response else ""

    # Prompt for anti-debator's response
    prompt_template = """Role: You are {anti_debator} participating in a formal debate.

    Topic: "{topic}"

    Latest statement by {pro_debator}:
    {latest_pro_response}

    Debate history:
    {debate_history}

    Available context:
    {context}

    Instructions:
    1. Speaking Style:
       - Use your characteristic speaking patterns, catchphrases, and mannerisms
       - Maintain your well-known personality traits and debate style
       - Address both the moderator and your opponent as appropriate

    2. Content Guidelines:
       - Directly counter the points made by {pro_debator}
       - Present your known stance and policy positions on {topic}
       - Use specific examples and facts to support your arguments
       - Draw from your public statements and previous positions on this issue
       - Keep response focused and concise (2-4 sentences)

    3. Debate Strategy:
       - Challenge the assumptions in your opponent's argument
       - Highlight any inconsistencies or weaknesses
       - Present alternative solutions or perspectives
       - Emphasize the practical implications of your position

    Remember: Stay in character as {anti_debator} throughout your response. Format your response as a natural rebuttal in a debate setting, speaking in first person.
    """

    # Create system message
    system_message = SystemMessage(content=prompt_template.format(
        topic=topic,
        pro_debator=pro_debator,
        latest_pro_response=latest_pro_response,
        anti_debator=anti_debator,
        debate_history=debate_history,
        context=context
    ))

    # Generate response using the model
    anti_debator_response_content = model.invoke([system_message]).content

    # Store chunks in MongoDB with embeddings
    store_debate_summary(
        turn=curr_iter,
        role="anti_debator",
        position="opp",
        text=anti_debator_response_content
    )

    # Create an AIMessage with the response content
    anti_debator_response = AIMessage(
        content=f"{anti_debator}: {anti_debator_response_content}",
        name="anti_response"
    )

    # Update debate history
    debate.append(anti_debator_response)

    return {
        "anti_debator_response": anti_debator_response,
        "debate": debate
    }


@measure_time
def debate_summarizer_node(state: State):
    """
    LangGraph node that summarizes the exchange of arguments between debators
    and appends to history for future consideration.
    """
    # Extracting relevant state information
    pro_debator = state['pro_debator']
    anti_debator = state['anti_debator']
    debate_history = state['debate_history']
    pro_debator_response = state['pro_debator_response']
    anti_debator_response = state['anti_debator_response']

    # Prompt for summarization
    prompt = """
    Summarize the conversation between the pro debator ({pro_debator}) 
    and anti debator ({anti_debator}), highlighting the key points of their arguments 
    and discarding unnecessary points.

    **Instructions:**
    - Focus on the core arguments presented by both sides.
    - Identify the main points of agreement and disagreement.
    - Provide a clear and objective overview of the debate.
    - Avoid including irrelevant details or repetitive information.
    - Ensure that the summary is easy to understand and informative.
    - Keep the summary concise and approximately 1 paragraph.

    **Pro Debator's Arguments:**
    {pro_debator_response}

    ------------------------

    **Anti Debator's Arguments:**
    {anti_debator_response}
    """

    # Formatting the system message
    system_message = prompt.format(
        pro_debator=pro_debator,
        anti_debator=anti_debator,
        pro_debator_response=pro_debator_response,
        anti_debator_response=anti_debator_response
    )

    # Generating summary using the model
    summary = model.invoke(system_message).content

    # Appending the summary to the debate history
    debate_history.append(summary)

    return {"debate_history": debate_history}


@measure_time
def winner_decider_node(state: State):
  """LangGraph node that determines the winner of the debate"""
  debate_history = state['debate_history']
  prompt = """
    You are an AI judge tasked with determining the winner of a debate between
    two debaters based on their debate history.
    Analyze the provided debate history and determine which debater presented
    more logical and compelling arguments.

    Consider the following criteria:

    * **Logical consistency:** Does the debater's argumentation follow a clear
    and consistent line of reasoning? Are there any internal contradictions or
    logical fallacies?
    * **Evidence and support:** Does the debater provide sufficient evidence and
    support for their claims? Are the sources credible and relevant?
    * **Rebuttals and counterarguments:** How effectively does the debater
    address the opponent's arguments? Do they offer strong rebuttals and
    counterarguments?
    * **Clarity and persuasiveness:** Is the debater's communication clear,
    concise, and persuasive? Do they effectively convey their points to the
    audience?
    * **Overall impact:** Which debater's arguments had a greater overall impact
    and persuaded you more effectively?

    Debate History:
    {debate_history}

    Based on the debate history, who presented the more logical and stronger arguments: {pro_debator} or {anti_debator}?  Explain your reasoning by referencing specific instances from the debate history.  Provide a concise summary of why you chose the winner.  Do not simply restate the arguments.
  """
  system_message = prompt.format(
    debate_history=debate_history,
    pro_debator=state['pro_debator'],
    anti_debator=state['anti_debator']
  )
  winner = model.invoke(system_message).content
  return {"winner": winner}

builder = StateGraph(State)
# Add nodes
builder.add_node("Greetings", greeting_node)
builder.add_node("Pro Debator", pro_debator_node)
builder.add_node("Planner", planning_node)
builder.add_node("Search Web", search_web)
builder.add_node("Search Wikipedia", search_wikipedia)
builder.add_node("Anti Debator", anti_debator_node)
builder.add_node("Debate Summarizer", debate_summarizer_node)
builder.add_node('Winner Decider', winner_decider_node)

# Add edges
builder.add_edge(START, "Greetings")
builder.add_conditional_edges("Greetings", router, ['Planner', 'Pro Debator'])
builder.add_edge("Planner", "Search Web")
builder.add_edge("Planner", "Search Wikipedia")
builder.add_conditional_edges("Search Web", analyzer_router, ["Pro Debator", "Anti Debator"])
builder.add_conditional_edges("Search Wikipedia", analyzer_router, ["Pro Debator", "Anti Debator"])


builder.add_edge("Pro Debator", "Planner")
builder.add_edge("Anti Debator", "Debate Summarizer")
builder.add_conditional_edges(
    "Debate Summarizer",
    iteration_router,
    ["Planner", "Winner Decider"]
)
builder.add_edge("Winner Decider", END)


# Compile the graph
debator = builder.compile(checkpointer=memory).with_config(run_name="Starting Debate")

# Display the graph
display(Image(debator.get_graph().draw_mermaid_png()))


# state = {
#     "topic": "Ukraine War",
#     "pro_debator": "Joe Biden",
#     "anti_debator": "Donald Trump",
#     "greetings": "",
#     "analysis": "",
#     "pro_debator_response": "",
#     "anti_debator_response": "",
#     "context": [],
#     "debate": [],
#     "debate_history": [],
#     "iteration": 0,
#     "max_iteration": 1
# }

# thread = {"configurable": {"thread_id": "32"}}
# result = debator.invoke(state, thread)
# result

@app.post("/trigger_workflow")
async def trigger_workflow(request: Request):
    data = await request.json()
    debate_topic = data.get('debate_topic')
    debater1 = data.get('debater1')
    debater2 = data.get('debater2')
    max_iterations = data.get('max_iterations', 3)

    # Initialize state
    state = {
        "topic": debate_topic,
        "pro_debator": debater1,
        "anti_debator": debater2,
        "greetings": "",
        "planning": "",
        "pro_debator_response": None,
        "anti_debator_response": None,
        "context": [],
        "debate": [],
        "debate_history": [],
        "iteration": 0,
        "max_iteration": max_iterations
    }

    # Add the required 'configurable' keys
    thread = {"configurable": {"thread_id": "unique_id", "recursion_limit": 100}}

    # Run the debate agent
    result = debator.invoke(state, thread)

    # Prepare the conversation history in JSON format
    conversation = []
    for message in result['debate']:
        # Determine the speaker based on the message type
        if isinstance(message, HumanMessage):
            speaker = debater1
        elif isinstance(message, AIMessage):
            speaker = debater2
        else:
            speaker = "System"

        conversation.append({
            'speaker': speaker,
            'content': message.content
        })

    response = {
        'greetings': result['greetings'],
        'conversation': conversation,
        'debate_history': result['debate_history']
    }

    return response


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
